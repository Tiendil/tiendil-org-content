---
title = "Пределы прогресса"
tags = ["theory", "neural-networks", "ai-notes-2024"]
series = "ai-notes-2024"
published_at = "2024-11-23T17:00:00+00:00"
seo_description = "TODO"
seo_image = ""
---

TODO: tags

Продолжаю заметки об ИИ на конец 2024 года.

/// brigid-series
tag = "ai-notes-2024"
///

Пришло время поговорить о самой хайповой, самой животрипещущей теме: доколе будет лететь текущий прогресс? Ждёт ли нас сингулярность в 2025 году или всё останется как прежде? Когда же наконец появится наш бог из металла и ~~силикона~~ кремния?

Есть большое искушением сказать что-то вдухе «да, будет сингулярность, пора строить алтари для процессоров» или «нет, закончилось уже всё, глупые машины останутся глупыми машинами».

Поддавшись этому искушению, я долго и безрезультатно пытался накатать эссе, но в итоге заключил, что нужно смотреть шире. Затрагиваемая область слишком большая, чтобы можно было ткнуть пальцем и сказать «всё идёт сюда». Точнее ткнуть-то можно — обосновать сложно. В частности потому, что одно и то же утверждение можно обосновывать по-разному, смотреть на него через разные призмы, каждая из которых будет показывать только часть правды. Захламлять же пост повторением одного и того же с разных точек зрения не хочется.

Поэтому я решил описывать не конечную точку, а ограничения на пути к ней, которые уже видны или логически вытекают из текущей ситуации.

Иными словами: очертить пределы прогресса.

<!-- more -->

## Рекогнистировка через призму поколений моделей

В прошлых постах мы обсудили два тезиса:

- Анализируя решения крупных разработчиков ИИ, таких как OpenAI или Google, мы можем делать достаточно точные предположения о состоянии этой области знаний.
- Весь текущий прогресс стоит на одной конкретной базовой технологии — генеративных базах знаний, которые есть большие статистические модели.

Давайте посмотрим, как эволюционировали топовые релизации этих моделей.

Идеальным примером станет серия моделей от OpenAI: каждая новая модель буквально соответствует этапу развития технологий, как я их вижу:

1. Юность — GPT-3 — до предела улучшаем модель с помощью масштабирования данных и эксплуатации железа.
2. Становление — GPT-4 — когда исчерпаны возможности экстенсивного развития, мы переходим на интенсивный путь максимальной адаптации архитектуры. Этот этап логично закончился мультимодальностью (поддержкой разных типов данных: текст, изображения, звук).
3. Зрелось — O1 — когда мы больше не можем **радикально** улучшать архитектуру целевой системы, мы начинаем строить надсистему, в которой оригинальная система является одним из компонентов. Шаблон [Chain-of-Thought](https://www.promptingguide.ai/techniques/cot), на который натюнена O1, как раз и является первой такой надсистемой, пусть и очень простой. Его можно воспринимать как последовательное применение модели к [чёрной доске](https://en.wikipedia.org/wiki/Blackboard_(design_pattern)). Следующим шагом, например, может быть мультиакторность и специализация моделей.
4. Старость (?) — GPT-5 — когда все возможности для радикального улучшения технологии исчерпаны, мы берём в руки напильник и начинаем долгий процесс её оптимизации. Улучшать технологию можно ещё долго и, накопительно, сделать её на порядки лучше, но эспотенциальный рост закончен. В частности, в интернетах упорно ходят слухи, что ждать большого рывка от GPT-5 не стоит.

Обращу ваше внимание на то, что изменение базовой модели — крайне дорогая операция. Модели не меняют по желанию левой пятки. Их меняют ровно тогда, когда выжимать что-то новое из старой модели становится экономически нецелесообразно по сравнению с вложением средств в мовый подход. Иными словами, когда достигнут предел быстрого развития старой модели.

Например, в какой-то момент стало нецелесорбразно вкладывать основные ресурсы в масштабирование данных и утилизацию железа, поэтому переключились на оптимизацию архитектуры. Когда архитектуру дотюнили, перенаправили финансовые потоки на эксперименты по созданию надсистемы.

Поэтому мы можем выдвинуть следующие гипотезы:

1. Исчерпан запас экстенсивного развития через масштабирование данных и железа. Если бы он не был исчерапан, продолжали бы выпускать GPT-3.X модели, их бы масштабировали для работы на 2,5,10,100500 видюхах одновременно, NVidia выпускала бы сверхоптимизированное железо для запуска сверхпрсотых, но гигантских нейронок, etc.
2. Исчерапан запас архитектурного развития. OpenAI, как и все остальные, не смогли за длительное время изобрести или купить технологию, которая позволила бы продолжать модернизировать архитектуру. Иначе вместо O1 OpenAI тренировала бы GPT-4.5 или GPT-5, заметно превосходящие GPT-4 по качеству.
3. Мы сейчас на этапе построения надсистем над генеративными базами знаний, поскольку именно подготовки таких моделей сейчас отдаётся предпочтение.

## Рекогнистировка через призму статистических моделей

Улучшать статистические модели можно несколькими путями:

1. Усложняя подготовку модели: больше данных, дольше обучение — лучше результаты.
2. Условжняя саму модель — изменяя архитектуру.
3. Специализируя модель: увеличиваем точность ограничивая область поиска решений.
4. Масштабируя модель горизонтально: статистически корректируем ошибки, создавая множество вариантов ответа. Если в двух из трёх запусков модель говорит А, а в одном — Б, то скорее всего правильный ответ — А.

Варианты 1, 2, 3 — пререготива разработчиков моделей, утрируя, они определяет их конечную форму.

Вариант 4 — пререготива пользователей моделей (не исключительно, но в основном), он не меняет форму моделей, но позволяет повышать точность их результатов задорого.

Держа в уме поколения моделей, мы можем видеть, что от вариантов 1 и 2 уже не ждут радикальных прорывов.

Вариант 3, скорее всего, не очень интересен разработчикам универсальных моделей, так как областей специализации бесконечно много, а значит радикально улучшать конечный продукт через серийное производство специализированных моделей не получится. Улучшения будут, но скорее сопоставимые с вариантами 1 и 2.

Зато на нём фокусируются разработчики с возможностями поменьше, заточенные на конечный продукт.

Идеальным примером третьего варианта будет [Suno](https://suno.com/) — сервис для генерации музыки и голоса для песен, значительно превосходящий по качеству работу универсальных моделей. Работать над подобными специализациями разработчикам универсальных моделей однозначно не выгодно: чтобы собрать данные и натренировать, условно, 100-1000 специализированных моделей (чтобы собрать на них универсальную мета-модель), нужно 100-1000 команд уровня Suno. Если же помнить что Suno — один стартап-лидер из многих (многие из которых умирают в безвестности), то оценка необходимых ресурсов должна вырасти ещё раз в 100.

Вариант 4 не даёт качественного скачка. Если, в какой-то области, модель ошибается немного, то горищонтальное масштабирование уберёт эту ошибку и ответы станут немного лучше (раз ошибка была небольшой). Если у модели слепое пятно в какой-то области, то это же слепое пятно, скорее всего, останется и после масштабирования.

ТУДУ: ????
Модель O1 можно считать удачной (?) попыткой в специализацию (вариант 3) универсальной модели.

Соответственно, мы можем продолжить выдвигать гипотезы:

4. Возможности генеративных баз более-менее определены — скорее всего они будут оставаться на уровне GPT-4 плюс-минус. Само собой, они будут становиться быстрее, меньше и т.п.
5. Разговоры про возможность продолжения быстрого прогресса через масштабирование вычислений на этапе эксплутации (вариант 4), скорее всего, маркетинговый булшит — понятно, что у некоторых людей большое искушение строить песчаные замки, вместо того чтобы трезво смотреть на вещи.ч



## Направления

- Надсистемы, но на сколько сложно получить качественный скачёк с ними? Может быть пропость между демонстрацией базовых улучшений и реальным качественным скачком. Любой профи отметит, что в его области есть огромные невидимый пласт знаний и навыков, который разделяет новичка и профессионала, нет никаких оснований полагать, что это не справедливо для мультиагентных систем. Что если для постройки реально работающей одной такой системы понадобится целый датацентр с миллионам одновременно работающих агентов? Кто сказал, что в нашем мозгу этих агентов меньше?
- Специализация, минификация, ускорение.
- Эволюционное (а не революционное) улучшение на всех фронтах.


### Концептуальных изменений в технологиях в ближашие 5-10 лет не будет

`Generative Knowledge Base` сформировалась как технология и она будет определять движение прогресса в ближайшие годы.

По следующим причинам:

1. `GKB` на текущий момент имеет огромный потенциал для улучшения и оптимизации. На мой взгляд, 2-3 порядка есть в запасе. Есть направления эволюционного улучшения для каждой части технологии, начиная от подготовки данных и заканчивая удешевлением эксплуатации. Вкладывание ресурсов в эти направления даёт быстрый и более-менее гарантированный результат. Поэтоу крупные игроки будут вкладывать ресурсы в эволюционные изменения, получая предсказуемый результат, который будет, в том числе, способствовать увеличению бонусов тех же директоров.
2. `GKB` позволяет решать множество задач, которые раньше были не решаемы, решались плохо или дорого. Эти задачи во многом видны, вкладывать ресурсы в их решение выгодно, вложения обещают большую отдачу. Все, кто не тренит свою большие модели будут вкладывать ресурсы в решение задач существующими моделями. Так как задач пока больше чем участников рынка и нет смысла искать что-то новое.
3. Разрабатывать радикально новые технологии очень дорого, рисковано и людей на это просто нет. Этим будут заниматься в академии, но ресурсов на ещё один прыжок веры, как сделала OpenAI с масштабированием обучения, никто не даст.

------

### Разница между «легко и понятно как сделать» и «непонятно и нелегко»

Очень быстро и очень медленно.

## Морфологический анализ vs Statistics

Полный перебор vs Интуиции. Науку движет полный перебор, ограниченный интуицуией. Не одна интуиция.

## Сильный ИИ требует дифференциации механизмов/архитектур, не одной общей архитектуры

## Пределы текущих технологий

## Устойчивый [сильный ИИ](https://ru.wikipedia.org/wiki/Сильный_и_слабый_искусственные_интеллекты) пока не ждём

Сейчас есть несколько шкал для оценки сильности ИИ, например [вот](https://builtin.com/artificial-intelligence/types-of-artificial-intelligence), но все они сырые и нераспространённые. По крайней мере, на википедии я отдельной статьи на тему не нашёл.


- Семантическая статистическая база данных, а не ИИ, только один из блоков
  - идут научные статьи (и около научные в блогах) о том что такое сознание и о том, что текущие архитектуры нейронок — это не то
  - ИИ будет совокупностью разных технологий, разныз блоков, из которых у нас сейчас есть только часть. И эта архитектура будет много сложнее, чем всё, что у нас сейчас есть.
  - Отсебя: люди мыслят образами, а не строят предложения переусложнёнными марковскими цепями.
  - Если вы просите статистическую модель дать вам варианты, она даст наиболее вероятные варианты. Если попросите дать экспериментальные/нестандартные варианты, она даст вам наиболее вероятные экспериментальные/нестандартные. Это одна из самых больших моих проблем во взаимодействии с нейронками. Потому что морфологический анализ — это не статистический алгоритм, это полный перебор.
  - Нет сильных аргументов в пользу того, что у нас есть всё, чтобы сделать ИИ. Даже все компоненты, не говоря уже об архитектуре.
  - Что такое сильный ИИ пока не понятно. (ТУДУ: уровни силы ИИ, где-то были)
  - По сути, весь наш прогресс в стронг ИИ за последние лет 50 — это созданные несколько модулей, которые симулируют его кусочки, но как их собрать вместе, чем склеить, до сих пор никто не знает.
  - Решение задач олимпиад - маркетинговый булшит
  - Нужно ещё несколько прорывов в базах данных (эмбедингах) и коммуникациях между нейронками. И железе клнечно.
  - Просто комбинирование моделей не поможет, равно как и прямолинейное заигрывание с памятью. Необходим формат/протокол/система обмена мета алгоритмами мышления. Например, если попросить человека сделать резюме текста, то разные люди будут делать его по-разному: кто-то перескажет, своими словами, кто-то сначала выпишет тезисы, потом соединит их в текст, кто-то будет удалять лишнее, пока не останется только суть. И для разных текстов могут быть выгодны разные подходы.
  - Технологией ии будут не нейронки (по крайней мере в текущем виде) а что-то поверх нейронок.
  - Дообучение через промпты увеличивает касество, но не снимает проблему крайних случаев.
  - OpenAI прыкрыла алайгмент лабу

- Рабочих рук не хватает даже на текущий фронт работ, и их неоткуда брать. Квалификация для реальной разработки работающих ИИ систем требуется выше, чем для среднего программиста.
- Специализация нейронок
- Реальных прорывных продуктов в эксплуатации нет. Кроме чатов, копилотов (что круто) и маркетинговых/развлекательных штук. Копилоты — это ещё один инструмент для людей, инструментов у нас много (автомобили, например). Он многое изменит, но не концептуально. Чаты подрывают поиск. Остальное подрывает развлечения.
- Текущие статистические модели создают отдельную нишу инструемнтов, как IoT, или как шейдеры в геймдве, или как смартфоны. Они становятся неотъемлемой частью нашей жизни и рынка, но только расшрят его, не подрыват/отменяют существующие инструменты.
- Перешли на новый уровень масштабирования (o1), его стало экономически эффективнее делать, чем масштабировать сами нейронки.
- Процесс иниеграции первичных нейронрк в софт: кады, иде, фоторедакторы. По сути только начался. Пока он полностью не провернётся, не пройдёт через эволюционный отбор, прогрессировать во что то брлее сложное нет смысла, т. К. Не видно направление.
- Результат всего этого будет, скорее всего, в том, что во многих областях деятельности «просто станет лучше»
- LLM не сможет выразить/передать ваш опыт в чём-то, потому что он у вас пока только в голове, а не в общечеловеской базе знаний, которой становятся LLM. Соответственно, специалистам, учёным и прочим лидерам всё ещё придётся писать тексты самостоятельно, если они хотят туда новизну добавить. LLM в этом смогут помочь, но не более того.
- Началась специализация нейронок в виде «LLM-name coder», «LLM-name math», etc.
- Мы всё ещё не видим реально прорывных рабочиз приложений, серебряных пуль, за исключением чатов и генерации развлекалова (картинки, музыка). Ну, feeds.fun конечно есть ещё :-)
- Изменения будут как если бы людям вместо каменных орудий труда выдали те же, но железные.
- Уже было несколько технологий, которые должны были «подорвать всё», но не подорвали.
- Сравнить с внедрением ИИ

------

Предел возможностей текущих технологий уже виден

Сильного ИИ пока не ждём

Сингулярность пока не ждём

Массового внедрения ИИ пока не ждём

----------------

- Соответственно, если хотите сделать следующий прорыв, надо концетрировать усилия на взаиомдействии моделей, а не на тюнинге одной модели.
- Я бы ставил следующие прорывы на синтез неронок и DSL
