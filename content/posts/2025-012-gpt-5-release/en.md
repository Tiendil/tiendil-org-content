---
title = "Summary of GPT-5 presentation without marketing pixie dust"
tags = [ "practice", "theory", "futurology", "neural-networks"]
published_at = "2025-08-08T12:00:00+00:00"
seo_description = "Summary of GPT-5 presentation without marketing pixie dust — progress is slowing down."
seo_image = "./lie-1.jpg"
---

/// brigid-images
caption = """Let he who thinks this is a coincidence cast the first stone at me.

For the record, in the scientific community, disputes over such plots can go as far as ostracizing the authors. But this is business, marketing — so it's fine, right?
"""

[[images]]
src = "./lie-1.jpg"
alt = """Someone "accidentally" doubled one of the bars in the chart."""

[[images]]
src = "./lie-2.jpg"
alt = "«Случайно» уменьшили один из столбцов на графике раза в три."
alt = """Someone "accidentally" reduced one of the bars in the chart by about three times."""
///

1. The LLM has become, on average, slightly (by a few percent) smarter.
2. In some aspects, the LLM has become significantly smarter (by tens of percent).
3. In some aspects, the LLM has become a bit dumber(!).
4. The API has become cheaper, or not, — it depends on how you use it.
5. OpenAI is deliberately misleading people about the capabilities of the new model — see the screenshots.

=> The world's LLM leader has begun to bog down, and the rest will likely follow.

When everything is going well and you make another breakthrough, you don't cheat with the pictures.

This doesn't mean that progress has stopped, but it does mean that the growth of technology is transitioning from the explosive phase of "discovering new things" to a more or less steady phase of "optimizing technologies in a million directions, where there are only enough hands for a hundred."

We are close to the "disillusionment" phase of the [Gartner hype cycle](https://en.wikipedia.org/wiki/Gartner_hype_cycle).

In this regard, I would like to remind you of my [AI future prognosis]{post:ai-notes-2024-prognosis} — so far, it's holding up.

Let's me add a few more thoughts.

<!-- more -->

Global projects like Stargate will not affect the situation to the extent their creators hope. The problem isn't a shortage of hardware or data centers, but the limitations of model and hardware architectures. These problems can't be solved by building more data centers — they're solved by scaling R&D through:

- training and hiring specialists;
- launching high-risk experiments;

The first point is more or less fine (the hype helps, and in 5–10 years we'll have plenty of young specialists), but the second one isn't. The current leaders have grown too big and too dependent on investors (in the West) and the state (in China) — they simply can't take risks. Neither OpenAI, nor Google, nor Meta can now make a sharp pivot toward any technology that is architecturally alternative to today's LLMs, no matter how promising it might be. For the next phase of explosive growth — which will come sooner or later, and may even lead to strong AI — we need "yet another OpenAI".

Почему крупные игроки не могут сделать разворот. Текущие технологии LLM уже приносят им деньги, в их оптимизацию залиты огромные бюджеты и их дальнейшая оптимизация и профит от неё предсказуемы, хоть и не обещают взрывного роста. Любая новая технология потребует сопоставимых вложений в оптимизацию, чтобы хотя бы достичь паритета с текущими LLM, при этом всегда будет большой риск ошибиться и выкинуть миллиарды в трубу.

Поэтому пока пределы оптимизации текущего железа и архитектуры LLM не будут исчерпаны прям полностью, в поиск альтернатив существенных денег вкладываться не будет. А до исчерпания направлений оптимизации ещё как до луны.

За примером ходить далеко не надо — взять те же видеокарты и параллельные вычисления.

Утрируя:

- Было ли всегда очевидно что массовые параллельные вычисления — это мощная и нужна штука? Конечно!
- Строили ли дорогие суперкомпьютеры на существующих технологиях, которые пытались в параллельные вычисления? Да!
- Разрабатывали ли архитектуры для параллельных вычислений? Десятилетиями!
- Но первое массовое по объёмам поставок железо с массовым параллелизмом появилось в узких нишах: для компьютерных игр, сложного рендера и науки. Потому, что именно в этих отраслях без него было вообще никуда. Только когда практика подтвердила правильность и верность пути, а сама технология тихонько преодолела детские болезни, началась её массовое внедрение. Например, браузеры начали использовать GPU при рендере примерно в 2010-ых.
- Была ли теоретическая возможность вложить больше миллиардов в разработку «видюх», чтобы получить сопоставимый результат на годы раньше через масштабирование R&D? Была, но только никому не нужен был подобный риск, когда всё и так работало и было множество направлений для более безопасного инвестирования в прогресс и прибыль.

С LLM сейчас то же самое происходит. Пока мы не переварим все возможности, которые они для нас открыли, появление чего-то более мощного возможно скорее ввиду счастливой случайности, чем в силу целенаправленных усилий.
