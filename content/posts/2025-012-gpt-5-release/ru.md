---
title = "Саммари презентации GPT-5 без маркетинговой мишуры"
tags = [ "practice", "theory", "futurology", "neural-networks"]
published_at = "2025-08-08T12:00:00+00:00"
seo_description = "Саммари презентации GPT-5 без маркетинговой мишуры — прогресс замедляется."
seo_image = "./lie-1.jpg"
---

/// brigid-images
caption = """Кто считает, что это случайность, пусть первым бросит в меня камень.

Если что, в том же научном сообществе разбирательство по поводу таких графиков может дойти вплоть до остракизма авторов. Но это ж бизнес, маркетинг — все так делают, правда?
"""

[[images]]
src = "./lie-1.jpg"
alt = "«Случайно» увеличили один из столбцов на графике в два раза."

[[images]]
src = "./lie-2.jpg"
alt = "«Случайно» уменьшили один из столбцов на графике раза в три."
///

1. Сеть стала, в среднем, немного (на проценты) умнее.
2. В некоторых аспектах сеть стала значительно умнее (на десятки процентов).
3. В некоторых аспектах сеть стала немного глупее(!).
4. API стало дешевле, или нет, — смотря как вы его используете.
5. OpenAI сознательно вводят людей в заблуждение по поводу способностей новой модели — смотрите скрины.

=> Мировой лидер в LLM начал вязнуть в болоте, скорее всего за ним последуют все остальные.

Когда у вас всё хорошо и вы совершаете очередной прорыв, вы не мухлюете с картинками.

Это не значит, что прогресс закончился, но это значит что развитие технологий переходит из взрывной фазы «открытия нового» в более-менее линейную фазу «оптимизации технологий в 100500 направлениях, когда рук хватает для только 100».

Близится фаза «разочарования» из [цикла хайпа](https://en.wikipedia.org/wiki/Gartner_hype_cycle).

В связи с этим напомню о своём [прогнозе о будущем ИИ]{post:ai-notes-2024-prognosis}  — пока сбывается.

Добавлю вот ещё что.

<!-- more -->

Глобальные проекты вроде Stargate не повлияют на ситуацию в той степени, в которой надеются их создатели. Так как проблема не в нехватке железа и датацентров, а в ограничениях архитектуры моделей и железа. Эти проблемы не решаются постройкой датацентров, они решаются расширением R&D через:

- подготовку и найм кадров;
- запуск рискованных экспериментов;

Если с первым всё почти нормально (хайп помогает, через 5-10 лет у нас будет много молодых спецов), то со вторым — нет. Текущие лидеры стали слишком крупными и зависимыми от инвесторов (на западе) и государства (в Китае) — они не в состоянии рисковать. Ни OpenAI, ни Google, ни Meta сейчас не могут сделать резкий разворот в пользу какой-либо технологии архитектурно альтернативной текущим LLM, какой-бы перспективной она ни была. Чтобы случился следующий этап взрывного роста (который рано или поздно случится и может таки приведёт к сильному ИИ) нужна «ещё одна OpenAI».

Почему крупные игроки не могут сделать разворот. Текущие технологии LLM уже приносят им деньги, в их оптимизацию залиты огромные бюджеты и их дальнейшая оптимизация и профит от неё предсказуемы, хоть и не обещают взрывного роста. Любая новая технология потребует сопоставимых вложений в оптимизацию, чтобы хотя бы достичь паритета с текущими LLM, при этом всегда будет большой риск ошибиться и выкинуть миллиарды в трубу.

Поэтому пока пределы оптимизации текущего железа и архитектуры LLM не будут исчерпаны прям полностью, в поиск альтернатив существенных денег вкладываться не будет. А до исчерпания направлений оптимизации ещё как до луны.

За примером ходить далеко не надо — взять те же видеокарты и параллельные вычисления.

Утрируя:

- Было ли всегда очевидно что массовые параллельные вычисления — это мощная и нужна штука? Конечно!
- Строили ли дорогие суперкомпьютеры на существующих технологиях, которые пытались в параллельные вычисления? Да!
- Разрабатывали ли архитектуры для параллельных вычислений? Десятилетиями!
- Но первое массовое по объёмам поставок железо с массовым параллелизмом появилось в узких нишах: для компьютерных игр, сложного рендера и науки. Потому, что именно в этих отраслях без него было вообще никуда. Только когда практика подтвердила правильность и верность пути, а сама технология тихонько преодолела детские болезни, началась её массовое внедрение. Например, браузеры начали использовать GPU при рендере примерно в 2010-ых.
- Была ли теоретическая возможность вложить больше миллиардов в разработку «видюх», чтобы получить сопоставимый результат на годы раньше через масштабирование R&D? Была, но только никому не нужен был подобный риск, когда всё и так работало и было множество направлений для более безопасного инвестирования в прогресс и прибыль.

С LLM сейчас то же самое происходит. Пока мы не переварим все возможности, которые они для нас открыли, появление чего-то более мощного возможно скорее ввиду счастливой случайности, чем в силу целенаправленных усилий.
