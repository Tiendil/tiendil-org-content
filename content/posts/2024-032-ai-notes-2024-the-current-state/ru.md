---
title = "Текущее состояние"
tags = ["theory", "neural-networks", "ai-notes-2024"]
series = "ai-notes-2024"
published_at = "2024-11-23T17:00:00+00:00"
seo_description = "TODO"
seo_image = ""
---

TODO: tags

Продолжаю заметки об ИИ на конец 2024 года.

/// brigid-series
tag = "ai-notes-2024"
///

В прошлых постах мы обсудили два тезиса:

- Анализируя решения крупных разработчиков ИИ, таких как OpenAI или Google, мы можем делать достаточно точные предположения о состоянии этой области знаний.
- Весь текущий прогресс стоит на одной конкретной базовой технологии — генеративных базах знаний, которые есть большие статистические модели.

Опираясь на эти тезисы, давайте посмотрим на текущее состояние индустрии.

<!-- more -->

## Рекогносцировка через призму поколений моделей

Первым делом, давайте посмотрим, как эволюционировали топовые релизации больших универсальных LLM моделей — главное достижение последних лет.

Идеальным примером станет серия моделей от OpenAI: каждая новая модель буквально соответствует этапу развития технологий, как я их вижу:

ТОДО: гпт о1 маленькая о
1. Юность — GPT-3 — до предела улучшаем модель с помощью масштабирования данных и эксплуатации железа.
2. Становление — GPT-4 — когда исчерпаны возможности экстенсивного развития, мы переходим на интенсивный путь максимальной адаптации архитектуры. Этот этап логично закончился мультимодальностью (поддержкой разных типов данных: текста, изображений, звука).
3. Зрелось — о1 — когда мы больше не можем **радикально** улучшать архитектуру целевой системы, мы начинаем строить надсистему, в которой оригинальная система является одним из компонентов. Шаблон [Chain-of-Thought](https://www.promptingguide.ai/techniques/cot), на который натренирована O1, как раз и является первой такой надсистемой, пусть и очень простой. Его можно воспринимать как последовательное применение модели к [чёрной доске](https://en.wikipedia.org/wiki/Blackboard_(design_pattern)). Следующим шагом, например, может быть мультиакторность и специализация моделей.
4. Старость (?) — GPT-5 — когда все возможности для радикального улучшения технологии исчерпаны, мы берём в руки напильник и начинаем долгий процесс её оптимизации. Улучшать технологию можно ещё долго и, накопительно, сделать её на порядки лучше, но взрывной рост закончен. В интернетах упорно ходят слухи, что ждать большого рывка от GPT-5 не стоит.

Обращу ваше внимание на то, что изменение базовой модели — крайне дорогая операция. Модели не меняют по желанию левой пятки. Их меняют ровно тогда, когда выжимать что-то новое из старой модели становится экономически нецелесообразно по сравнению с вложением средств в новый подход. Иными словами, когда достигнут предел быстрого развития старой модели.

Например, в какой-то момент стало нецелесорбразно вкладывать основные ресурсы в масштабирование данных и утилизацию железа, поэтому переключились на оптимизацию архитектуры. Когда архитектуру дотюнили, перенаправили финансовые потоки на эксперименты по созданию надсистемы.

Поэтому мы можем выдвинуть следующие предположения:

1. Осталось мало возможностей для взрывного экстенсивного развития через масштабирование данных и железа. Если бы он(ТОДО!) не был исчерапан, продолжали бы выпускать GPT-3.X модели, их бы масштабировали для работы на 2, 5, 10, 100500 видюхах одновременно, NVidia выпускала бы сверхоптимизированное железо для запуска сверхпростых, но гигантских нейронок, etc.
2. Осталось мало возможностей для взрывного архитектурного развития. OpenAI, как и все остальные, не смогли за длительное время изобрести или купить технологию, которая позволила бы продолжать модернизировать архитектуру. Иначе вместо O1 OpenAI тренировала бы GPT-4.5 или GPT-5, заметно превосходящие GPT-4 по качеству.
3. Мы сейчас на этапе построения надсистем над генеративными базами знаний, поскольку именно подготовке таких моделей сейчас отдаётся предпочтение.

## Рекогнистировка(ТОДО!) через призму статистических моделей

Улучшать статистические модели можно несколькими путями:

1. Усложнять подготовку модели: больше данных, дольше обучение — лучше результаты.
2. Усложнять саму модель — изменять архитектуру.
3. Специализировать модель — увеличивать точность ограничивая область поиска решений.
4. Масштабировать модель горизонтально — корректировать ошибки, создавая множество вариантов ответа. Самый простой вариант: если в двух из трёх запусков модель говорит А, а в одном — Б, то скорее всего правильный ответ — А. Чуть сложнее: запуск нескольких специализированных моделей, каждая из которых решает часть задачи.

Пути 1, 2, 3 определяют конечную форму модели, поэтому они — прерогатива разработчиков моделей.

Вариант 4 не изменяет форму самой модели, но позволяет управлять точностью её ответов, поэтому он больше подходит для пользователей моделей.

Держа в уме поколения моделей, мы можем предположить, что от вариантов 1 и 2 уже не ждут радикальных прорывов.

Идеальным примером третьего варианта будет [Suno](https://suno.com/) — сервис для генерации музыки и песен, значительно превосходящий по качеству работу универсальных моделей. Работать над подобными специализациями разработчикам универсальных моделей не выгодно: чтобы собрать данные и натренировать, условно, 100-1000 специализированных моделей (и упаковать их в универсальную мета-модель), нужно 100-1000 команд уровня Suno. Если же помнить, что Suno — один стартап-лидер из многих (погибших в безвестности), то оценка необходимых ресурсов должна вырасти ещё раз в 100.

Вариант 4 не даёт качественного скачка. Если, в какой-то области, модель ошибается немного, то горизонтальное масштабирование уберёт эту ошибку и ответы станут немного лучше (раз ошибка была небольшой). Если у модели слепое пятно в какой-то области, то это же слепое пятно, скорее всего, останется и после масштабирования.

В этом плане, модель O1 выглядит как попытка «задёшево» двинуть универсальную LLM одновременно по путям 3 и 4. Получилось лучше, чем GPT-4, но не на порядок. Например, я всё ещё пользуюсь своими [кастомными GPT-шками]{post:my-gpts} вместо O1 для специфических задач.

Соответственно, мы можем продолжить выдвигать гипотезы:

4. Возможности генеративных баз более-менее определены — скорее всего они будут оставаться на уровне GPT-4 плюс-минус. Само собой, они будут становиться быстрее, меньше, немного точнее и т.п.
5. Разговоры про возможность продолжения быстрого прогресса через масштабирование вычислений на этапе эксплутации (вариант 4), скорее всего, маркетинговый ~~булшит~~ ход для поддержания хайпа и потока инвестиций.

## Рекогнистировка (ТОДО!) через имзменения на рынках

- ChatGPT 3 вышла летом 2020 — 4 года назад.
- ChatGPT 4 вышла весной 2023 — 1.5 года назад.

На мой взгляд, прошло достаточно времени чтобы делать первые выводы о перспективах технологии: в каких местах она меняет правила игры, в каких делает лучше, а в каких не меняет ничего.

Обратите внимание:

- Всё выше написанное — это моё личное субъективное мнение.
- Всё написанное ниже — это моё ещё более личное и более субъективное мнение. Это не результат исследования, а продукт моего опыта и наблюдений за новостями.

### Подрываемые рынки

Прямо сейчас идут большие изменения в следующих областях:

- **Персональные ассистенты** — LLM чаты на порядки улучшают функциональность всего, что было до них: от вордовской скрепки до умных колонок, меняя правила взаимодействия пользователей с ними.
- **Профессиональный софт** — [IDE](https://en.wikipedia.org/wiki/Integrated_development_environment), [CAD](https://en.wikipedia.org/wiki/Computer-aided_design), графические редакторы — весь профессиональный софт, который хоть как-то формализровал свою область — а это весь крупный профессиональный софт. Профессиональные области становятся на порядок доступнее новичкам, а профессионалы становятся на порядок эффективнее. При этом меняются концепции работы с профессиональным софтом: от директивного к диалоговому. Пока не понятно, где остановятся изменения: будет ли это просто отдельный режим или вся разработка перейдёт в чат, но прежними редакторы точно не останутся.
- **Поиск** — пока широко не заметно, но большинство людей пользующихся ChatGPT или аналогами отмечают, что обращаются к классическому поиску значительно реже. На мой взгляд, незаметность обусловлена сочетанием огромной пользовательской базы поисковиков и пока ещё малой распространнённостью чатов. Было бы интересно посмотреть на график количество запросов в гугл от среза пользователей вроде «ИТ-шники из Флориды».
- **Музыка** — ряд стартапов, например, [suno](https://suno.com/) продемонстрировал что генерировать музыку и песни по формализованному заданию значительно проще, чем изображение и видео. Стулья под охранителями интеллектуальных прав шатаются, но те пока держатся. Скрестим пальчики за будущее. Кстати, если задуматься, генерацию музыки можно отнести к профессиональному софту: ноты и разметка текстов — это типичные [DSL](https://en.wikipedia.org/wiki/Domain-specific_language).

### Улучшаемые рынки

В некоторых областях всё становится просто лучше, например:

- Croudsourcing платформы вроде [Талаки](https://toloka.ai/) переориентируются с людей на ИИ, но не меняют свою суть.
- Сервисы модерации, анализа настроений, фильтрации контента тоже становятся лучше, не меняясь концептуально.
- No-code платформы. Для меня странно записывать их сюда, но де-факто ничего революционного в них я не видел, хотя AI явно увеличивает их возможности.
- Обработка текста: перевод, «техническая журналистика». Определённо становится удобнее, но ту же автоматизацию написания технических заметок о спортивных матчах, судах и событиях на бирже я не готов считать за подрыв чего-то.
- Обучение — LLM находят своё применение, но пока не меняют ничего радикально. Например, нет примеров платформ или школ, в которых ИИ заменил бы учителя.

### Рынки, на которых все ждут изменений, а их всё нет

В некоторых областях быстрого прорыва не случилось, хотя многие ждали:

- Робототехинка — штучные гуманоидные роботы вроде даже как есть, но они занимают всё те же ниши развлечений для богатых и вычурного маркетинга.
- Игры — не вышло ни одной крупной игры с next-gen NPC или next-gen процедурной генерацией. Нет даже ничего, что сравнилось бы с известными прецендентами из before-deep-learning эпохи, вроде [Creatures](https://en.wikipedia.org/wiki/Creatures_(video_game_series)), [Black and White](https://en.wikipedia.org/wiki/Black_%26_White_(video_game)), etc. Последнее очень подозрительно, либо свидительствует об очень долгих циклах адаптации технологий в геймдеве, либо о концептуальных недостатках технологии.
- Профессиональная генерация ready-to-view художественного контента — генерировать контент по точной детальной спецификации всё ещё нельзя и даже близко не заметно когда станет возможным. Работа профессиональных хужожников, операторов и всех причастных всё ещё нужна и ценна.
- Профессиональная генерацией ready-to-read художественного контента. Аналогичная ситуация с графикой.
- Скорость внедрения самоходных повозок радикально не изменилась.
- Медицина — прорывных прорывов пока не заметно, доступность медицины «для бедных» пока не улучшается.
- Наука — [за AlphaFold дали Нобелевку](https://www.nature.com/articles/d41586-024-03214-7), но я пока не встречал новостей в духе «учёные с помощью AlpaFold сделали что-то революционное» (про это же и Nature пишет). AlphaFold, на мой взгляд, находится ближе к области профессионального софта, чем базовой штуки, подрывающией науку. Периодически встречают статьи про автоматизацию исследований с помощью LLM, но пока не слышал об их практическом применении.
- Бюрократия — пока не было примеров крупной автомтизации работы с бумажками в государственном аппарате.

Основываясь на этих наблюдениях, добавим ещё несколько гипотез:

6. ИИ изменяет нашу жизнь к лучшему, но не делает это радикально: улучшения не происходят везде, не происходят быстро, самые сильные изменения имеют строгую локализацию в области профессионального софта и развлечений, тем самым не затрагивая напрямую жизнь большинства людей.
7. Существует ряд областей, которые ещё «стоят в очереди» за дарами ИИ и могут рвануть, но это возможность пока скорее гипотетическая, чем реальная.
