---
title = "Математики добрались до нейронных сетей"
tags = [ "neural-networks", "theory", "scientific-papers", "science"]
published_at = "2020-12-09T12:00:00+00:00"
seo_description = "Исследователи утверждают, что нейронные сети, обученные методом градиентного спуска, близки такой штуке как kernel machines."
seo_image = ""
---

На [arxiv.org](https://arxiv.org/) выложен интересный препринт: [Every Model Learned by Gradient Descent Is Approximately a Kernel Machine](https://arxiv.org/abs/2012.00152).

Как видно из названия, исследователи утверждают, что нейронные сети, обученные методом градиентного спуска (один из самых распространённых вариантов обучения) близки такой штуке как [kernel machines](https://en.wikipedia.org/wiki/Kernel_method) — одной из техник машинного обучения «предыдущего поколения».

У kernel machines есть несколько интересных особенностей:

- Техника хорошо проработана математически.
- Требует значительно менее дорогих вычислений.
- Вместо «выделения» признаков «напрямую» использует обучающую выборку.

Из этого может неслучиться несколько интересных вещей.

- «Готовая» математика упростит сети и/или улучшит их результат и/или ускорит/удешевит обучение.
- Область возможностей сетей очертится более чётко — окажется, что они не выделяют никакие новые признаки, а используют только «запутанные» данные из обучающей выборки.

Оба варианта выглядят довольно интересно.

[Обсуждение на ycombinator.com](https://news.ycombinator.com/item?id=25314830)
