---
title = "LLM мыслят вширь, люди мыслят вглубь"
tags = ["theory", "development", "thinking", "neural-networks"]
published_at = "2026-02-11T12:00:00+00:00"
seo_description = "Почему LLM мыслят вширь, а люди мыслят вглубь и какие проблемы это создаёт при работе с LLM."
seo_image = ""
---

**LLM мыслят вширь, люди мыслят вглубь**

Сформулировал свою главную концептуальную претензию к LLM на текущий момент.

В чатах эта проблема заметна меньше — она стирается тем, что идёт постоянное общение с человеком, который направляет и корректирует LLM.

Зато очень заметна при vibe coding или когда спрашиваешь LLM что-то что предполагает абстрактный и большой ответ.

<!-- more -->

Когда человек ищет решение проблемы, он выбирает цель и методически движется к ней, отрезая всё лишнее от области поиска по мере движения. Надо реализовать X — будет реализовано X. Надо разбить вопрос по категориям, будут выбраны ортогональные категории (по мере возможностей человека) и вопрос разложится по ним.

Но поскольку LLM — это [вероятностные базы данных](https://tiendil.org/ru/posts/ai-notes-2024-generative-knowledge-base), а у вероятности нет направления, LLM по умолчанию генерирует ответ во всех возможных направлениях.

Даже если используется итерационный вывод (любого рода), каждая итерация всё равно пытается двигать ответ во всех направлениях и, из-за вероятностной природы, загрязняет ответ неверными и/или ненужными ответвлениями.

То есть проблема остаётся даже при [итерационной корректировке ошибки](https://tiendil.org/ru/posts/life-and-work-with-mistakes).

Это приводит к тому, что меняется сама структура результата — он получается запутанным, неортогональным. Это уже само по себе плохо, но есть дополнительные сложности:

- Это не тот формат ответа, который люди (ок, профессионалы) привыкли получать — его надо воспринимать и перерабатывать по-другому — более образно, менее аналитически — требуется отдельный навык.
- Анализ такого ответа делать сложнее из-за неортогональности и множества мелких отклонений — очень легко пропустить критическое отклонение, которое локально не выглядит значимым, но приводит к существенным последствиям при попадании в работу.

В итоге появляется интересный вопрос: решится ли эта проблема просто уменьшением длительности итераций вывода и увеличения их количества, или есть качественные отличия между работой мозга человека и LLM?

Под качественными отличиями я имею в виду что-нибудь вроде встроенной символьной логики на уровне архитектуры нейронной сети.
