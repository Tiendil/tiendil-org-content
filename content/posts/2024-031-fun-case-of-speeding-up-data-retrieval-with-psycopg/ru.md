---
title = "Интересный случай оптимизации извлечения данных с помощью Psycopg"
tags = ["practice", "development", "python", "backend", "feeds-fun", "interesting", "databases"]
published_at = "2024-11-22T12:00:00+00:00"
seo_description = "Неожиданные подводные камни при работе с Psycopg и как их можно обходить, на конкретном примере."
seo_image = "cover.png"
---

/// brigid-images
src = "./cover.png"
caption = "Скорость извлечения данных из базы для каждой из оптимизаций. В процентах от скорости базовой реализации. Обратите внимание, что количество извлекаемых строк слабо влияет на время выполнения."
///

Раз в год-два мне приходится вспоминать, что Python, ммм… не C++. Обычно, это происходит внезапно, как в этот раз.

~~Проведя вдумчивый анализ~~ Надоело по 10 секунд ждать загрузки новостей в [feeds.fun](https://feeds.fun/), поэтому я засучил рукава и полез оптимизировать. Сходу чуть не взялся за эпический рефакторинг, но вовремя вспомнил, что сначала надо измерить, а потом уже резать. Мерять в данном случае совет буквальный — взял профайлер — [py-spy](https://github.com/benfred/py-spy) — и посмотрел, что конкретно тормозит.

Оказалось, тормозит не вся логика, а вполне конкретное место с извлечением из PostgreSQL таблицы ~100000 строк, плюс-минус 20000. Индексы на месте, тесты проводил с базой на RAM-диске, поэтому со стороны базы всё должно было быть ок.

Такому количеству строк не удивляйтесь:

- Во-первых, у меня большой поток новостей.
- Во-вторых, для каждой новости читалка сейчас ставит около 100 тегов.

Вооружившись py-spy и исодникакми [psycopg](https://github.com/psycopg/psycopg), я прошёл через три этапа оптимизации, **уменьшив время выполнения целевой функции примерно в 4 раза только за счёт изменения формата запрашиваемых колонок в SELECT запросе и кода обработки результата**.

Далее я расскажу про последовательность любопытных открытий, которые сделал в процессе.

/// attention | Внимание!
Этот пост — не исследование производительностие Psycopg или Python, а описание конкретного опыта на конкретной задаче со специфическими данными.
///

<!-- more -->

## Оригинальная задача

Сокращённое описание таблицы, в которой хранятся данные (убрал лишнее, чтобы не ехало форматирование):

```
ffun=# \d o_relations
                                       Table "public.o_relations"
   Column   |           Type           | Collation | Nullable |
------------+--------------------------+-----------+----------+
 id         | bigint                   |           | not null |
 entry_id   | uuid                     |           | not null |
 tag_id     | bigint                   |           | not null |
 created_at | timestamp with time zone |           | not null |
Indexes:
    "o_relations_pkey" PRIMARY KEY, btree (id)
    "idx_o_relations_entry_id_tag_id" UNIQUE, btree (entry_id, tag_id)
```

**Задача проблемной Python функции:** извлечь все `tag_id` для переданного списка `entry_id` и вернуть словарь со множеством `tag_id` для каждого `entry_id`.

Результат функции должен быть примерно такой:

```
{
    "uuid-1": {1, 2, 3, 4, 5},
    "uuid-2": {1, 7, 8},
    ....
}
```

Никакой магии, один `SELECT` плюс формирование словаря.

## Оговорки и описание тестов

Чтобы избежать влияния сторонних факторов, я слегка упростил оригинальную задачу:

- Вместо передачи списка `entry_id` я передавал количество записей, которые нужно извлечь (1000, 10000, 100000).
- Не использовал фабрику строк Psycopg [dict_row](https://www.psycopg.org/psycopg3/docs/api/rows.html#psycopg.rows.dict_row), чтобы исключить лишние преобразования.
- Замеряемые функции синхронны, оригинальный код был асинхронным.
- Тестовые данные брал с прода.

Также обратите внимание:

- Измерялось время выполнения Python функции с необходимым преобразованием данных, а не чистое время работы Psycopg. Так как важна именно скорость получения необходимого результата.
- Пробовал явно включать использование бинарного протокола коммуникации с PostgreSQL, но изменения были незаметны, поэтому далее об этом варианте не говорю.
- Перед измерением каждая из тестовых функций выполнялась 1 раз, чтобы прогреть базу.
- Для измерения каждой функции я делал 100 запусков и усреднял результаты.

Среднее время выполнения функции было:

- Для базовой версии: 2.28, 23.18, 227.91 секунд для 1000, 10000, 100000 записей соответственно.
- Для финальной версии: 0.58, 5.83, 57.27 секунд для 1000, 10000, 100000 записей соответственно.

Но помните, задача и данные специфичны для конкретного проекта, и даже для моего конкретного аккаунта в нём, поэтому мало что скажут стороннему читателю.

/// details | Полный код теста
```
--8<-- "./optimizations.py"
```
///

Базовая версия функции:

```
--8<-- "./optimizations.py:version_1"
```

## Оптимизация 1

Первое, что показал профайлер — большое время проведённое кодом в [psycopg/types/datetime.py](https://github.com/psycopg/psycopg/blob/master/psycopg/psycopg/types/datetime.py) — больше 18%! Как вы могли заметить, никакой работы со временем в коде функции нет.

«Ага» — сказал я себе — «Зря ты, Tiendil, звёздочку в SELECT поставил, но тебе только две колонки надо, а время парсить всегда дорого».

И заменил звёздочку на конкретные колонки:

```
--8<-- "./optimizations.py:version_2"
```

## Оптимизация 2

Следующий запуск профайлера показал, что стало лучше, но всё-ещё много времени тратится на парсинг `UUID` — типа колонки `entry_id`.

Как я уже упоминал, особенность данных в том, что для одного значения `entry_id` может быть порядка 100 записей в таблице. Поэтому нет смысла парсить `entry_id` для каждой строки.

Что если запрашивать `entry_id` как строку, а парсить уже на стороне Python, но только один раз для каждого уникального значения?

```
--8<-- "./optimizations.py:version_3"
```

## Оптимизация 3

Стало ещё лучше (смотрите заглавную картинку).

На этот раз py-spy завёл меня в куда более интересное место: [psycopg/pq/pq_ctypes.py](https://github.com/psycopg/psycopg/blob/master/psycopg/psycopg/pq/pq_ctypes.py), а точнее в [PGresult.get_value](https://github.com/psycopg/psycopg/blob/d38cf7798b0c602ff43dac9f20bbab96237a9c38/psycopg/psycopg/pq/pq_ctypes.py#L925-L934).

`PGresult.get_value` возвращает одно значение из результата запроса по номеру строки и колонки. Попутно в нём происходит конвертация данных из формата C в формат Python, в частности, с помощью вызова [ctypes.string_at](https://docs.python.org/3/library/ctypes.html#ctypes.string_at).

Так вот, преобразование данных из формата C в формат Python — очень дорогое удовольствие. Даже не так, ОЧЕНЬ ДОРОГОЕ удовольствие. Особенно когда этих преобразований много, например, по два на каждую из 100000 строк.

Можно ли сократить количество преобразований? Конечно, давайте на стороне базы собирать колонки в одну, а на стороне Python разбирать их обратно.

Сказано — сделано, вот наш финальный вариант:

```
--8<-- "./optimizations.py:version_4"
```

Результаты этой версии примерно в 4 раза быстрее базовой (на моих данных).

/// attention | На всякий случай проговорю словами
Отформатировать и склеить колонки результата на стороне PostgreSQL и распарсить Python-ом может быть быстрее, чем запрашивать колонки как отдельные значения.
///

По крайней мере это справедливо для конкретного случая использования Psycopg. Я люблю эту библиотеку, поэтому думаю, что у альтернатив ситуация не лучше.
