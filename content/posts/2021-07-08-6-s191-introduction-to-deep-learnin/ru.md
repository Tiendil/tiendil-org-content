---
title = "MIT 6.S191: галопом по Deep Learning"
tags = [ "neural-networks", "education", "practice",]
published_at = "2021-07-08T12:00:00+00:00"
description = "Курс MIT 6.S191: Introduction to Deep Learning — отличная возможность познакомиться с темой."
seo_image = ""
---

[Отмучавшись с матаном]{post:how-to-teach-and-not-teach-math}, я решил, что времени на основательное разбирательство со всем машинным обучением уйдёт слишком много — надо срезать углы.

Поэтому следующей целью выбрал курс MIT [6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/).

Потому что [MIT](https://www.mit.edu/) и по темам лекций видно широкое покрытие темы.

Курсом очень доволен.

<!-- more -->

Сложные вещи излагаются доступно и кратко:

- Из матана потребовалось только знание [chain rule](https://en.wikipedia.org/wiki/Chain_rule) — правила нахождения производной композиции функции. И то — опционально.
- Из линейной алгебры — базовые операции над матрицами и векторами.
- Из теории вероятностей — базовые знания.

Правда с теорией вероятностей нюанс есть. Курс действительно понятен с базовыми знаниями, но только, если некоторые утверждения принимать на веру. Чтобы понять сложные места необходимо глубокое знание теории, которого у меня нет. И, что печально, я затрудняюсь определить время, необходимое на его получение.

На прохождение потребовалось 6 дней:

- 3 дня по 2 лекции и лабораторной работе;
- 2 дня на подробное разбирательство со сделанными лабами: пройтись внимательно по каждой строчке кода, покрутить параметры;
- день на оставшиеся лекции.

Лекции лежат на youtube, задания и решения лаб на github. Курсу несколько лет, каждый год записывают новые версии лекций. Можно посмотреть динамику изменения предметной области :-)

Курс затрагивает следующие темы:

- Введение в нейронные сети: нейрон, перцептрон, etc.
- RNN — [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) — моделирование последовательностей.
- CNN — [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network), GAN — [Generative Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network) —  обработка и генерация изображений.
- Разные типы обучения: [supervised](https://en.wikipedia.org/wiki/Supervised_learning), [unsupervised](https://en.wikipedia.org/wiki/Unsupervised_learning), [reinforcement](https://en.wikipedia.org/wiki/Deep_reinforcement_learning).
- Борьбу с предвзятостью моделей из-за искажений данных и/или алгоритмов. Классический пример: модель определяет чернокожих как преступников, потому что в обучающей выборке большинство преступников было с чёрной кожей.

Последнему пункту уделяется много времени и рассказываются действительно интересные штуки. Без повесточки :-)

Раскрыта важность топологии данных, архитектуры сетей. Есть примеры нетривиальных (как мне, дилетанту, кажется) архитектурных решений.

Рассматриваемые примеры сетей, конечно, не rocket science, но и не классические экземпляры из туториалов «мамкиных датасаентистов».

Недостатки тоже есть:

- В лабораторных работах не хватает автоматических проверок правильности промежуточного кода. Есть места где пишешь, пишешь и не понимаешь правильная у тебя логика или уже нет. Если итоговая модель не работает, не всегда можно определить проблемное место и надо проверять всё с начала.
- В лабораторных работах хотелось бы видеть больше информации об используемых библиотеках, особенно о принципах контроля размерностей данных.
- Последние лекции читаются удалённо гостями курса — практиками, качество записи и подачи материала у них хромает.

Я бы сказал, что этот курс — отличная возможность познакомиться с темой, даже если вы не планируете ей заниматься и не хотите делать лабораторные.
