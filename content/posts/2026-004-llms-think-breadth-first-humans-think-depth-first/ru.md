---
title = "LLM мыслят вширь, люди мыслят вглубь"
tags = ["theory", "development", "thinking", "neural-networks"]
published_at = "2026-02-11T12:00:00+00:00"
seo_description = "Почему LLM мыслят вширь, а люди мыслят вглубь и какие проблемы это создаёт при работе с LLM."
seo_image = "./cover.jpg"
---

/// brigid-images
src = "cover.jpg"
alt = "Обложка поста (c) ChatGPT"
///

Сформулировал свою главную концептуальную претензию к LLM на текущий момент. На основе моего личного опыта.

В чатах эта проблема заметна меньше — она стирается тем, что идёт постоянное общение с человеком, который направляет и корректирует LLM.

Зато очень заметна при vibe coding или когда спрашиваешь LLM что-то что предполагает абстрактный и большой ответ.

<!-- more -->

Когда человек ищет решение проблемы, он выбирает цель и методически движется к ней, отрезая всё лишнее от области поиска по мере движения. Надо реализовать X — будет реализовано X. Надо разбить вопрос по категориям, будут выбраны ортогональные категории (по мере возможностей человека) и вопрос разложится по ним.

Но поскольку LLM — это [вероятностные базы данных]{post:ai-notes-2024-generative-knowledge-base}, а у вероятности есть нюансы с направлением, LLM генерирует ответ во всех возможных направлениях.

Даже если используется итерационный вывод (любого рода), каждая итерация всё равно пытается двигать ответ во всех направлениях и, из-за вероятностной природы, загрязняет ответ неверными и/или ненужными ответвлениями.

То есть проблема остаётся даже при [итерационной корректировке ошибки]{post:life-and-work-with-mistakes}.

Это приводит к тому, что меняется сама структура результата — он получается запутанным, неортогональным. Это уже само по себе плохо, но есть дополнительные сложности:

- Это не тот формат ответа, с которым люди (ок, профессионалы) привыкли работать. Его надо воспринимать и перерабатывать по-другому — более образно, менее аналитически — требуется отдельный навык.
- Анализ такого ответа делать сложнее из-за неортогональности и множества мелких отклонений — очень легко пропустить критическое отклонение, которое локально не выглядит значимым, но приводит к существенным последствиям при попадании в работу.

В итоге появляется интересный вопрос: **решится ли эта проблема уменьшением длительности итераций вывода и увеличения их количества, или есть качественные отличия между работой мозга человека и LLM?**

Под качественными отличиями я имею в виду что-нибудь вроде встроенной символьной логики на уровне архитектуры нейронной сети.
