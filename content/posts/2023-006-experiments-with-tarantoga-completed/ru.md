---
title = "Эксперименты закончены"
tags = [ "practice", "reflection", "tarantoga", "theory", "exocortex", "tarantoga-development"]
series = "tarantoga-development"
published_at = "2023-05-08T12:00:00+00:00"
seo_description = "Подведение итогов разработки пет-проекта — персональной базы знананий с мультиагентной системой."
seo_image = ""
---

Заметки о разработке [Тарантоги]{tags:tarantoga} — экзокортекса для управления информацией:

/// brigid-series
tag = "tarantoga-development"
///

Чтоб я ещё когда взялся делать blackboard мультиагентную систему с темпоральными свойствами :-D

В этом посте будет краткое подведение итогов, чтобы закрыть вопрос.

Заметок по ходу эксериментов я не делал, поэтому какие-то интересные детали рассказать сложно — в голове всё смешалось и затёрлось уже.

<!-- more -->

## О чём текст

Все посты на тему Тарантоги находятся по тегу [Тарантога]{tags:tarantoga}.

Если кратко, то пробовал сделать универсальный [экзокортекс]{post:exocortex-3-5} — агрегатор всей личной информации + поисковик + умный организатор/менеджер информации (её слияние, определение дополнительных свойства, etc) с умным откатом изменений.

На практике это должно было бы выглядеть так:

- Забираем историю всех чатов, социальных сетей, новостных лент, лайков на github, писем, etc.
- Определяем свойства этой информации, связываем её между собой.
- Выделяем общие сущности: людей, места, события.
- Поверх этого ставится поисковик.
- Делаем API для подключения независимых агентов, которые с этой информацией работают как со [школьной доской](https://en.wikipedia.org/wiki/Blackboard_system): обогащают чем-то новым или реагируют на изменения.
- В случае ошибок или проблем с агентами, идём в историю изменений доски и откатываем их проблемную часть, после чего агенты всё пересчитывают по-новой.

## Финальный результат

Было 5 подходов. Последний можно назвать довольно успешным, учитывая амбициозность планов.

В итоге Tarantoga v0.5 можно описать так:

- Blackboard база знаний.
- С blackboard работает множество агентов.
- Агенты записывают утверждения (statements) в базу.
- База автоматически объединяет утверждения в концепты (concepts). По принципу общности сущности, то есть утверждения об X объединяются в концепт X.
- Утверждения и концепты — множества фактов вида `<имя, значение>`. Например: `name: xxx`, `description: yyy`, `url: zzz`.
- База знает какого типа может быть значение факта и как эти значения мержить, если есть несколько фактов с одинаковым именем.
- Для каждого утверждения можно (и нужно) указывать причинно-следственные связи с другими концептами. Инами словами, каждый агент может сказать: «я утверждаю, что a=b, c=d, основываясь на концептах X и Y».
- Благодаря таким связям, если мы удалим любое утверждение, то специализированный агент удалит все утверждения и концепты, которые были основаны на нём (выведены из него). После этого агенты заново пересчитают состояние базы.

Примерный перечень агентов в демо:

- Скачивание url.
- Парсинг html.
- Очистка базы — удаление зависимых концептов при удалении утверждения.
- Извлечение источников новостей из OPML файла.
- Извлечение новостей из RSS лент.
- Извлечение текста из HTML.
- Определение свойств/тегов контента из HTML (несколько агентов).
- Определение свойств/тегов контента из текста.
- Распространение определённых свойств по связанным сущностям.
- Создание утверждений/концептов новостей. Контент может быть в разной форме, она определяет как с ним работают, например, просматривают в GUI. Одни агенты вытягивают контент в разном виде из разных мест, другие парсят и нормируют, третьи преобразуют его в формат, пригодный для работы человека с Тарантогой.
- Определение рейтинга новости по её свойствам.

Ко бэкенду был прикручен GUI на [vue.js](https://vuejs.org/) с навигацией и поиском по базе. В лучших традициях, GUI настраивается данными из той же базы знаний.

Формально, это всё даже работает. Не без проблем, конечно.

## Почему решил прекратить эксперименты

Сложно и долго.

- Слишком большой объём работы для одного человека.
- Много работы требуется на решение технических вопросов: производительность, поиск/индексирование, оптимизация памяти. Такая работа не даёт выхлоп в виде фич, а для меня фичи первичны в данном случае.
- Не получилось найти высокоуровневые абстракции для данных, из-за чего приходится делать лишнюю работу. Со временем утряслось бы, но сильно много сил надо для этого.
- Трушная мультиагентность без фиксированной схемы данных — сложная штука: возникает рассинхрон агентов, гонки, etc. Особенно в сочетании с возможностью отката изменений.
- ChatGPT — новый инструмент предполагает новый подход ко всему. Тарантога, в том виде, в котором я его видел, мог устареть раньше, чем стал бы приносить пользу.

В итоге: слишком экспериментально и слишком низкоуровнево, чтобы эффективно вести разработку.

## Некоторые выводы

### Общее

- Не делайте универсальные штуки с нуля. Если только для хобби или обучения (как в данном случае). Универсальная штука требует сделать огромное количество предположений, многие из них окажутся неверными. Поэтому сначала решаем кусочки задачи, разбираемся с каждым по-отдельности, а потом объединяем их в общее решение.
- Не пишите тесты для прототипов. Только если для очень-очень критичной части. Время отъедает неимоверно, а затем выкидывается.
- Не делайте blackboard систему, если не уверены, что она вам нужна. Blackboard не такая простая, как кажется. Для меня это был не закрытый университетский гештальт, вряд ли это так для вас. В целом, я даже не знаю известных blackboard систем, которые были бы не продуктом научных лабораторий.
- Если таки делаете blackboard систему, то формализуйте/фиксируйте схемы данных (тем самым, ограничивайте операции агентов), иначе будет сложно понимать происходящее и поддерживать зоопарк агентов.
- Если можно не поддерживать темпоральность (любого вида, от истории операций, до поиска в прошлом) — не поддерживайте. Упростите проект на несколько порядков.

### Обошёлся без DSL для запросов к базе

Опробовал интересный подход к извлечению данных с сервера.

Вместо одного запроса а-ля «самописный SQL с блэкджеком и JSON», сделал набор простых команд — операций над множествами.

Выглядит так:

- Найти концепты с такими-то свойствами и поместить во множества A.
- Найти концепты с другими свойствами и поместить во множества B.
- C = A - B
- Загрузить данные концептов из множества C.
- Вернуть множества C, B и загруженные концепты.

Клиент формирует список и шлёт на сервер. Сервер делает всю магию и возвращает результат.

Таким образом не надо загоняться по поводу сложного DSL, правил обработки и оптимизации запросов. Все команды элементарные, оптимизации делаются в явном виде руками на уровне запроса. Очень удобно при прототипировании, не надо отвлекаться на непрофильные активности.

## Что дальше

В области хитрых экзокортексов я пожалуй, пока, всё. Подожду что ИИ грядущий нам готовит.

Но проблему свою я не решил, поэтому за март-апрель на коленке собрал RSS читалку с бэком, фронтом и ChatGPT. Сеть ставит кучу тегов каждой новости, а GUI умеет по ним фильтровать и сортировать с учётом правил от пользователя.

Буду потихоньку опенсорсить.
